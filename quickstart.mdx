---
title: 'Quick Start Guide'
description: 'Get started with Hi SDK - LLM Development Kit for Raspberry Pi'
---

## Installation Options

### Option 1: Pre-built Raspberry Pi Image

Download our pre-configured Raspberry Pi image with all dependencies:
[Download Hi SDK Image](your_drive_link_here)

<Note>
  This is a pre-built image with all dependencies installed on a Raspberry Pi OS 6.6.31+rpt-rpi-v8.
</Note>

### Option 2: Development Setup

<Info>
  **Prerequisites:**
  - Python 3.11.2 or higher
  - Raspberry Pi OS
</Info>

```bash
# Clone the repository
git clone https://github.com/svenplb/hi-sdk.git
cd hi-sdk

# Create and activate virtual environment
python3 -m venv --system-site-packages venv
source venv/bin/activate

# Install in development mode
pip install -e .
```

### Option 3: Manual Setup

If you prefer to set up dependencies manually on your existing system:

<Steps>
  <Step title="Install System Dependencies">
    ```bash
    sudo apt-get update
    sudo apt-get install -y python3-pip git
    ```
  </Step>

  <Step title="Install Ollama">
    ```bash
    curl https://ollama.ai/install.sh | sh
    ollama pull gemma:2b
    ```
  </Step>

  <Step title="Set Up SDK">
    ```bash
    git clone https://github.com/svenplb/hi-sdk.git
    cd hi-sdk
    python3 -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
    pip install -e .
    ```
  </Step>

  <Step title="Start API Server">
    ```bash
    python3 -m uvicorn main:app --reload
    ```
  </Step>
</Steps>

## Basic Usage

### Python SDK

```python
from sdk.client import HiClient
from sdk.utils import SDKLogger

# Initialize client
client = HiClient()

# Load a model
client.load_model("gemma2:2b", temperature=0.7)

# Simple chat
response = client.chat("Tell me a joke")
print(response)

# Streaming chat with callback
def on_token(token):
    print(token, end="", flush=True)

client.register_callback("on_token", on_token)
for _ in client.chat("Tell me another joke"):
    pass  # Tokens handled by callback
```

### CLI Usage

After installation, you can use the CLI:

```bash
# Start interactive chat
hi chat --model gemma2:2b --temp 0.7

# List available models
hi models
```

## Available Models

<CardGroup cols={3}>
  <Card title="qwen:0.5b" icon="feather">
    Ultra-lightweight model for basic tasks
  </Card>
  <Card title="qwen:1.8b" icon="bolt">
    Balanced performance (default)
  </Card>
  <Card title="gemma2:2b" icon="rocket">
    Enhanced capabilities for complex tasks
  </Card>
</CardGroup>

## Project Structure

```bash
hi-sdk/
├── sdk/
│   ├── __init__.py
│   ├── cli.py
│   ├── client.py
│   ├── utils.py
│   └── exceptions.py
├── examples/
│   └── quickstart.py
├── tests/
│   └── test_api.py
└── setup.py
```

<Note>
  We suggest using a virtual environment (we preinstall all dependencies on the device at the default path `/home/hi-sdk/venv`)
</Note>
```

The key changes include:
1. Proper frontmatter with title and description
2. Consistent heading hierarchy
3. Use of MDX components like `<Note>`, `<Info>`, `<Steps>`, and `<CardGroup>`
4. Proper code block language specifications
5. Organized structure following the Mintlify documentation style
